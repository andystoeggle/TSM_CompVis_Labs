{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A practical introduction to image classification\n",
    "In this notebook, we introduce image classification on a difficult dataset.\n",
    "## The CIFAR dataset\n",
    "\n",
    "You can download the data from here: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "In particular, we are using the CIFAR-10 dataset in the Python version:\n",
    "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz (About 160 MB)\n",
    "\n",
    "Untar the file in a directory named \"cifar-10\" placed in the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pathlib\n",
    "ddir = pathlib.Path(\"cifar-10\")\n",
    "print(f\"We expect the data files in directory {ddir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ddir/\"batches.meta\", 'rb') as fo:\n",
    "    labelnames = pickle.load(fo)[\"label_names\"]\n",
    "\n",
    "data_tr = []\n",
    "labels_tr = []\n",
    "for i in range(1, 6):\n",
    "    with open(ddir/\"data_batch_{}\".format(i), 'rb') as fo:\n",
    "        alldata = pickle.load(fo, encoding='bytes')\n",
    "    data_tr.append(alldata[b\"data\"])\n",
    "    labels_tr.append(np.array(alldata[b\"labels\"]))\n",
    "data_tr = np.vstack(data_tr)\n",
    "labels_tr = np.hstack(labels_tr)\n",
    "\n",
    "with open(ddir/\"test_batch\", 'rb') as fo:\n",
    "    alldata = pickle.load(fo, encoding='bytes')\n",
    "data_te = alldata[b\"data\"]\n",
    "labels_te = np.array(alldata[b\"labels\"])\n",
    "\n",
    "# %% We now have two datasets, one for training and one for testing\n",
    "\n",
    "print(\"Training data and labels with shapes: \", data_tr.shape, labels_tr.shape)\n",
    "print(\"Testing data and labels with shapes: \", data_te.shape, labels_te.shape)\n",
    "print(\"Meaning of the label codes: \", list(enumerate(labelnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Support function for drawing an image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def vec2im(imvector):\n",
    "    \"\"\" Convert a vector to an image \"\"\"\n",
    "    im = np.reshape(imvector, (3, 32, 32))  # reshape\n",
    "    # we want channels to be the last dimension, not the first.\n",
    "    im = np.transpose(im, axes=(1, 2, 0))\n",
    "    return im\n",
    "\n",
    "\n",
    "def showim(im, l=None, ax=None):\n",
    "    \"\"\" Displays the image im (1D vector with 3072 elements)\n",
    "    in the currently active axes, and set the title to the name\n",
    "    corresponding to label l (if given)\"\"\"\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "    ax.imshow(im)\n",
    "    if l is not None:\n",
    "        ax.set_title(labelnames[l])\n",
    "\n",
    "\n",
    "# %% Let's visualize some of our training images\n",
    "for i in range(10):\n",
    "    fig, ax = plt.subplots(figsize=(4, 4), dpi=80)\n",
    "    showim(vec2im(data_tr[i]), labels_tr[i], ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: implement a nearest-neighbor classifier approach in pixel space\n",
    "Given a testing image, we want to look for the training image that is _closest_ when comparing the raw pixel values.\n",
    "Remember that an image in our dataset is actually a 1D array with 3072 uint8 elements (i.e. 32x32x3 pixels, arranged in a line).\n",
    "\n",
    "### 1.1: define a function that given two images, computes the euclidean distance between them\n",
    "Each image is a point in 3072 dimensions.  You can use [`np.linalg.norm(...)`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html) to find the length of a 3072-dimensional vector...\n",
    "\n",
    "Remember that you should cast both vectors to `float` before subtracting them.  Use [`ndarray.astype(float)`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(imvec1,imvec2):\n",
    "    \"\"\" Returns the euclidean distance between two vectors im1 and im2 \"\"\"\n",
    "    return np.sum((imvec1.astype(float)-imvec2.astype(float))**2)**0.5\n",
    "\n",
    "def distance(imvec1,imvec2):\n",
    "    \"\"\" Returns the euclidean distance between two vectors im1 and im2 \"\"\"\n",
    "    return np.linalg.norm(imvec1.astype(float)-imvec2.astype(float))\n",
    "    \n",
    "print(\"Should be 1:\", distance(np.array([0,0]), np.array([0,1])))\n",
    "print(\"Should be 5:\", distance(np.array([4,0,0]), np.array([0,0,3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Given a testing image, find the nearest training instance\n",
    "Then print the true class of the testing instance, and the predicted class (i.e. the class of the closest training instance).\n",
    "\n",
    "Do this for each of the first 20 testing images. Should take at most 1 second per image.\n",
    "\n",
    "**Extension**: for each of the first 20 testing images, display it along with its class; besides it display the closest training image, and also the training image that is farthest away (use `fig,(ax1,ax2,ax3) = plt.subplots(ncols=3)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_im):\n",
    "    distances = []\n",
    "    for train_im in data_tr:\n",
    "        distances.append(distance(train_im, test_im))\n",
    "    return labels_tr[np.argmin(distances)]\n",
    "\n",
    "i = 3\n",
    "print(\"Predicted label: \", labelnames[classify(data_te[i,:])])\n",
    "print(\"True label: \", labelnames[labels_te[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Compute the accuracy of your classifier over 100 random testing instances\n",
    "How does it compare to the accuracy of a baseline classifier that just returns a random class (assuming that the 10 classes are equally distributed)? \n",
    "\n",
    "**Extension**: also compute the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def showim(im, l=None, ax=None):\n",
    "    \"\"\" Displays the image im (1D vector with 3072 elements)\n",
    "    in axis ax (or a new figure if not given),\n",
    "    and set the title to the name\n",
    "    corresponding to label l (if given)\"\"\"\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "    ax.imshow(im)\n",
    "    ax.axis(\"off\")\n",
    "    if l is not None:\n",
    "        ax.set_title(labelnames[l])\n",
    "\n",
    "# %% Let's look for the closest (and farthest) training image to each of the first 20 testing images\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for testi in tqdm.tqdm(np.random.randint(len(data_te), size=(2000,))):\n",
    "    imvec = data_te[testi]\n",
    "    l = labels_te[testi]\n",
    "    # im is now our test image, and l is the ground truth label\n",
    "    \n",
    "    d = np.full(len(data_tr), np.nan)\n",
    "    # for each training image, we'll save its distance from the query\n",
    "    for i in range(len(data_tr)):\n",
    "        d[i] = distance(data_tr[i], imvec)\n",
    "    i_closest = np.argmin(d)  # this is the index of the closest image\n",
    "    i_farthest = np.argmax(d)  # index of the farthest image\n",
    "\n",
    "    #print(\"Ground truth label: \", labelnames[l])\n",
    "    #print(\"Estimated label: \", labelnames[labels_tr[i_closest]], flush=True)\n",
    "\n",
    "    y_true.append(l)\n",
    "    y_pred.append(labels_tr[i_closest])\n",
    "\n",
    "    if(False):\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(14, 6), dpi=150)\n",
    "        # draw the test image (and label) on the first axis\n",
    "        showim(vec2im(data_te[testi]), l=labels_te[testi], ax=ax1)\n",
    "        # draw the closest image (and label) on the second axis\n",
    "        showim(vec2im(data_tr[i_closest]), l=labels_tr[i_closest], ax=ax2)\n",
    "        # draw the farthest image (and label) on the third axis\n",
    "        showim(vec2im(data_tr[i_farthest]), l=labels_tr[i_farthest], ax=ax3)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "acc = np.mean(y_true == y_pred)\n",
    "cm = sklearn.metrics.confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Visualize the confusion matrix as an heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm/np.sum(cm,axis=1,keepdims=True)*100, annot=True, xticklabels=labelnames, yticklabels=labelnames)\n",
    "plt.title(f\"Confusion Matrix on {len(y_true)} testing instances (acc={acc*100}%)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: implement and evaluate a feature extraction approach\n",
    "We are now going to extract a few features for each image (both training and testing). Then, we will train a classifier from the sklearn library on the training data, and evaluate it on testing data.\n",
    "\n",
    "### 2.1: implement a simple feature extractor\n",
    "Implement a function that given an image, returns the average value for red, green and blue in that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(im):\n",
    "    assert(im.shape[2] == 3)\n",
    "    #return np.random.rand(2)\n",
    "    #return np.concatenate([np.mean(im[:16,:,:], axis=(0,1)), np.mean(im[16:,:,:], axis=(0,1))])\n",
    "    return np.array(np.mean(im, axis=(0,1)))\n",
    "    \n",
    "testim = np.array([\n",
    "    [[1,2,3],[2,3,4]],\n",
    "    [[0,1,2],[1,2,3]],\n",
    "])\n",
    "print(testim.shape)\n",
    "print(\"Should return [1 2 3]: \",extract_features(testim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: compute features for the whole training set and testing set\n",
    "Build an array `features_tr` with shape `(50000,3)` containing the features for each training image (one per row).\n",
    "\n",
    "Build an array `features_te` with shape `(10000,3)` containing the features for each testing image (one per row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tr = []\n",
    "for vec in data_tr:\n",
    "    im = vec2im(vec)\n",
    "    features_tr.append(extract_features(im))\n",
    "features_tr = np.array(features_tr)\n",
    "\n",
    "features_te = []\n",
    "for vec in data_te:\n",
    "    im = vec2im(vec)\n",
    "    features_te.append(extract_features(im))\n",
    "features_te = np.array(features_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: train a sklearn classifier on the training set, and compute its accuracy on the testing set\n",
    "\n",
    "You can use any classifier, such as a K-Nearest-Neighbors classifier:\n",
    "\n",
    "To create:\n",
    "`clf = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)`\n",
    "\n",
    "To train:\n",
    "`clf.fit(features_tr, labels_tr)`\n",
    "\n",
    "To use it on new images\"\n",
    "`predicted_labels_te = clf.predict(features_te)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "import sklearn.neighbors\n",
    "import sklearn.ensemble\n",
    "#clf = sklearn.ensemble.RandomForestClassifier()\n",
    "#clf = sklearn.neighbors.KNeighborsClassifier(n_neighbors=15)\n",
    "clf.fit(features_tr, labels_tr)\n",
    "labels_pred = clf.predict(features_te)\n",
    "acc = np.mean(labels_te == labels_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "cm = sklearn.metrics.confusion_matrix(y_true=labels_te, y_pred=labels_pred)\n",
    "print(cm)\n",
    "\n",
    "# Visualize the confusion matrix as an heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, xticklabels=labelnames, yticklabels=labelnames, fmt='03d')\n",
    "plt.title(f\"Confusion Matrix on {len(y_true)} testing instances (acc={acc*100}%)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: extensions\n",
    "\n",
    "- Also compute the confusion matrix.\n",
    "- Explore how the accuracy changes if we use more than 1 neighbor.\n",
    "- When using more than one neighbor (try using many, e.g. 21), the classifier can also return *class probabilities*; read the documentation for [`clf.predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.predict_proba).  For each of the first 20 testing images, dispay the testing image and a bar plot of the probability for the 3 most probable classes for that image according to our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5: try extracting different/better features, and check how the accuracy changes\n",
    "\n",
    "Consider the different options proposed below.  For each, compute on the test set the accuracy of the classifier learned from the training set.\n",
    "\n",
    "- two random numbers (2 features).  Which accuracy do we expect?\n",
    "- five random numbers (5 features).  Which accuracy do we expect?\n",
    "- the average brightness of the image -- i.e. the mean value (1 feature)\n",
    "- the average RGB values in the top half and in the bottom half (6 features)\n",
    "- the mean value and variance of the image in each of the four quadrants (8 features)\n",
    "- the mean value and variance of the intensity of the gradient of the image in each of the four quadrants (8 features)\n",
    "- the concatenation of all above features\n",
    "- (optional) the Histogram of Oriented Gradients feature descriptor, as computed by the function `skimage.feature.hog(im)`.  [Read more](https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html#sphx-glr-auto-examples-features-detection-plot-hog-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: train a simple feedforward neural network to solve the classification task\n",
    "\n",
    "The code below defines a classic feedforward NN architecture with many 3072 input neurons, one hidden layer with 100 neurons, and 10 output neurons, used for classification.  Train the network for a few epochs and check its validation accuracy.\n",
    "\n",
    "What if you increase the number of neurons in the hidden layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=data_tr[0].shape))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "labels_tr_oh = keras.utils.to_categorical(labels_tr, num_classes=10)\n",
    "labels_te_oh = keras.utils.to_categorical(labels_te, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data_tr.astype(float)/255, labels_tr_oh, epochs=100,\n",
    "          batch_size=32,\n",
    "          validation_data=(data_te.astype(float)/255, labels_te_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "Train the network for a while, then for each of the first 20 testing images, dispay the testing image and a bar plot of the probability for the 3 most probable classes for that image according to our network.\n",
    "\n",
    "To apply the trained model, use [`Model.predict(...)`](https://keras.io/models/model/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(data_te.astype(float)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(out, axis=1) == labels_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trailer for next week: train a simple feedforward convolutional neural network to solve the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "images_tr = []\n",
    "for vec in data_tr:\n",
    "    images_tr.append(vec2im(vec))\n",
    "images_tr = np.array(images_tr)\n",
    "\n",
    "images_te = []\n",
    "for vec in data_te:\n",
    "    images_te.append(vec2im(vec))\n",
    "images_te = np.array(images_te)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=images_tr[0].shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "labels_tr_oh = keras.utils.to_categorical(labels_tr, num_classes=10)\n",
    "labels_te_oh = keras.utils.to_categorical(labels_te, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(images_tr.astype(float)/255, labels_tr_oh, epochs=100, batch_size=32,\n",
    "          validation_steps = None,\n",
    "          validation_data=(images_te.astype(float)/255, labels_te_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = model.predict(images_te.astype(float)/255)\n",
    "for testi in np.random.choice(10000, 20):\n",
    "    p = y_proba[testi,:]\n",
    "    print(p)\n",
    "    ix = np.argsort(p)[:-4:-1] # indices of the 3 largest elements\n",
    "    \n",
    "    fig,(ax1,ax2) = plt.subplots(ncols=2)\n",
    "    showim(vec2im(data_te[testi]), l=labels_te[testi], ax=ax1)\n",
    "    ax2.bar(x=[1,2,3], height=p[ix])\n",
    "    ax2.set(ylim=[0,1],\n",
    "            xticks=[1,2,3],\n",
    "            xticklabels=[labelnames[i] for i in ix],\n",
    "            title=\"3 most probable classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics\n",
    "l_true = []\n",
    "l_pred = []\n",
    "for testi in range(len(y_proba)):\n",
    "    l_pred.append(np.argmax(y_proba[testi,:]))\n",
    "    l_true.append(labels_te[testi])\n",
    "\n",
    "cm = sklearn.metrics.confusion_matrix(y_true=l_true, y_pred=l_pred)\n",
    "print(cm)\n",
    "\n",
    "# Visualize the confusion matrix as an heatmap\n",
    "import seaborn as sns\n",
    "acc = np.mean(np.array(l_pred)==np.array(l_true))\n",
    "sns.heatmap(cm, annot=True, xticklabels=labelnames, yticklabels=labelnames)\n",
    "plt.title(f\"Confusion Matrix on {len(l_true)} testing instances (acc={acc*100}%)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "w = model.layers[0].get_weights()[0]\n",
    "fig,axs = plt.subplots(nrows=w.shape[2], ncols=w.shape[3], figsize=(25,5))\n",
    "for r,c in itertools.product(range(w.shape[2]), range(w.shape[3])):\n",
    "    axs[r,c].imshow(w[:,:,r,c], cmap=\"gray\")\n",
    "    axs[r,c].axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
